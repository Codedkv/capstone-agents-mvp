# ğŸ¤– Proactive Business Analytics & Anomaly Detection System (MVP)

**Multi-Agent Architecture for Autonomous Business Operations Analysis**

---

## ğŸ“‹ Project Overview

This is a **multi-agent system** that automatically analyzes business operations, detects anomalies and bottlenecks, and provides **actionable recommendations** for optimization.

### Problem Statement
- Companies struggle to identify operational bottlenecks and performance anomalies across multiple data sources
- Manual analysis is **slow**, **fragmented**, and **reactive**
- **Solution:** Autonomous multi-agent system that continuously monitors metrics, discovers patterns, and proactively recommends interventions

### Key Features (MVP Phase)
âœ… **Coordinator Agent** â€” Orchestrates all sub-agents  
âœ… **Historical Analyzer** (mock) â€” Detects anomalies in historical trends  
âœ… **Live Monitor** (mock) â€” Real-time metric monitoring and alerts  
âœ… **Recommender** (mock) â€” Generates actionable, prioritized recommendations  
âœ… **Session Management** â€” Persistent memory and context tracking  
âœ… **Structured Logging** â€” Complete observability and tracing  
âœ… **Metrics Collection** â€” Performance monitoring  

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        User Query / Business Context            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Coordinator Agent         â”‚
    â”‚  (Orchestrator)            â”‚
    â”‚  - Task Distribution       â”‚
    â”‚  - Result Synthesis        â”‚
    â”‚  - Session Management      â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚        â”‚        â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”
    â”‚ Hist â”‚ â”‚ Live â”‚ â”‚ Recomâ”‚
    â”‚ Anal â”‚ â”‚ Mon  â”‚ â”‚ mend â”‚
    â”‚ yzer â”‚ â”‚ itor â”‚ â”‚ er   â”‚
    â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
        â”‚        â”‚        â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
             â”‚       â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
        â”‚  Session Memory â”‚
        â”‚  & Logging      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Final Report       â”‚
    â”‚  - Ranked Issues    â”‚
    â”‚  - Recommendations  â”‚
    â”‚  - Next Actions     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Responsibilities

| Agent | Role | Pattern |
|-------|------|---------|
| **Coordinator** | Routes tasks, synthesizes results | Sequential Orchestrator |
| **Historical Analyzer** | Time-series analysis, anomaly detection | Tool-using Agent |
| **Live Monitor** | Real-time metric monitoring, alerts | Polling/Threshold Agent |
| **Recommender** | Generates solutions, explains reasoning | Reasoning/Planning Agent |

---

## ğŸ“ Project Structure

```
capstone-project/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ ARCHITECTURE.md              # Detailed architecture & design decisions
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agents.py          # Base classes & interfaces
â”‚   â”œâ”€â”€ coordinator.py          # Main Coordinator Agent
â”‚   â””â”€â”€ mock_agents.py          # Mock sub-agents (MVP placeholders)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ session_manager.py      # Session & memory management
â”‚   â””â”€â”€ logging_system.py       # Structured logging & metrics
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ agent_config.yaml       # Agent configurations
â”‚   â””â”€â”€ tool_registry.json      # Tool definitions
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_business_metrics.csv  # Demo dataset
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py          # Unit & integration tests
â”‚   â””â”€â”€ conftest.py             # Pytest fixtures
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ capstone_demo.ipynb     # Kaggle Notebook (demo)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ test_cases.json         # Test scenarios
â”‚   â””â”€â”€ metrics_report.json     # Performance metrics (generated)
â””â”€â”€ docs/
    â””â”€â”€ DEPLOYMENT_ROADMAP.md   # Future scaling strategy
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone or download the project
git clone https://github.com/your-username/capstone-project.git
cd capstone-project

# Install dependencies
pip install -r requirements.txt

# Required packages:
# - google-generativeai (Gemini API)
# - pydantic (data validation)
# - pandas (data processing)
# - pytest (testing)
# - pyyaml (config parsing)
```

### 2. Configuration

Edit `config/agent_config.yaml` to customize agent parameters:

```yaml
agents:
  coordinator:
    model: "gemini-2.0-flash"
    temperature: 0.5
    max_tokens: 4096
```

### 3. Run the MVP

**Option A: Python Script**

```python
import asyncio
from agents.coordinator import CoordinatorAgent
from agents.mock_agents import (
    HistoricalAnalyzerAgent,
    LiveMonitorAgent,
    RecommenderAgent
)
from agents.base_agents import AgentConfig
from core.session_manager import SessionManager, BusinessContext
from core.logging_system import StructuredLogger, MetricsCollector

# Initialize components
logger = StructuredLogger(verbose=True)
metrics = MetricsCollector()
session_manager = SessionManager(storage_dir="./sessions")

# Create Coordinator
coord_config = AgentConfig(
    name="Coordinator",
    agent_type="coordinator"
)
coordinator = CoordinatorAgent(coord_config, session_manager, logger, metrics)

# Register sub-agents
coordinator.register_sub_agent(
    "historical_analyzer",
    HistoricalAnalyzerAgent(AgentConfig(name="HistoricalAnalyzer", agent_type="analyzer"))
)
coordinator.register_sub_agent(
    "live_monitor",
    LiveMonitorAgent(AgentConfig(name="LiveMonitor", agent_type="monitor"))
)
coordinator.register_sub_agent(
    "recommender",
    RecommenderAgent(AgentConfig(name="Recommender", agent_type="recommender"))
)

# Execute analysis
async def main():
    context = {
        "company_name": "Acme Corp",
        "analysis_period": "Q4 2024",
        "metrics_file": "data/sample_business_metrics.csv"
    }
    
    response = await coordinator.execute(
        "Analyze Q4 business performance and identify bottlenecks",
        context
    )
    
    print("Analysis Complete!")
    print(f"Status: {response.status}")
    print(f"Issues Found: {response.result['total_issues_found']}")
    print(f"Recommendations: {len(response.result['recommendations'])}")
    print(f"Execution Time: {response.execution_time_ms:.1f}ms")

asyncio.run(main())
```

**Option B: Jupyter Notebook (Kaggle)**

See `notebooks/capstone_demo.ipynb` for complete interactive demo.

### 4. Run Tests

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=agents --cov=core

# Run specific test
pytest tests/test_agents.py::TestCoordinatorAgent -v
```

---

## ğŸ“Š Output Format

### Final Report Structure

```json
{
  "analysis_completed_at": "2024-11-21T10:30:00Z",
  "total_issues_found": 2,
  "issues_by_severity": {
    "critical": 0,
    "high": 1,
    "medium": 1,
    "low": 0
  },
  "ranked_issues": [
    {
      "id": "hist_0",
      "source": "Historical Analysis",
      "description": "Sales drop by 34%",
      "severity": "high",
      "detected_at": "2024-11-21T10:30:00Z"
    }
  ],
  "recommendations": [
    {
      "priority": 1,
      "issue": "Sales drop on 2024-11-15",
      "action": "Investigate customer complaints and market trends",
      "estimated_impact": "High",
      "timeline": "Immediate (within 24 hours)",
      "assigned_to": "Sales Manager"
    }
  ],
  "next_actions": [
    "Review top-priority issues immediately",
    "Implement recommended actions",
    "Monitor progress and feedback"
  ]
}
```

---

## ğŸ” Logging & Observability

All agent actions are logged to `./logs/agent_trace.jsonl`:

```json
{"timestamp": "2024-11-21T10:30:00Z", "level": "INFO", "message": "[Coordinator] ğŸ”„ Starting coordinated analysis"}
{"timestamp": "2024-11-21T10:30:01Z", "level": "INFO", "message": "[HistoricalAnalyzer] Analyzing historical trends..."}
```

**Export metrics for analysis:**

```python
metrics.export_metrics("evaluation/metrics_report.json")
logger.export_logs("evaluation/logs_export.jsonl")
```

---

## ğŸ“ˆ Session Management

Sessions are automatically saved to `./sessions/{session_id}.json`:

```json
{
  "session_id": "session_20241121_103000_abc12345",
  "created_at": "2024-11-21T10:30:00Z",
  "business_context": {
    "company_name": "Acme Corp",
    "analysis_period": "Q4 2024"
  },
  "memory": {
    "historical_findings": [...],
    "live_alerts": [...],
    "recommendations": [...],
    "user_feedback": {}
  },
  "action_history": [...],
  "state": "completed"
}
```

**Resume a session:**

```python
session = session_manager.load_session("session_20241121_103000_abc12345")
print(f"Session {session.session_id} restored: {session.state}")
```

---

## ğŸ§ª Testing

### Unit Tests

- âœ… AgentResponse structure and serialization
- âœ… SessionManager CRUD operations
- âœ… Coordinator initialization and sub-agent registration
- âœ… Mock agent execution and output validation
- âœ… Logging and metrics collection
- âœ… Error handling and edge cases

### Integration Tests

- âœ… End-to-end Coordinator workflow
- âœ… Sub-agent collaboration
- âœ… Session persistence and recovery
- âœ… Full system execution with mock agents

**Run tests:**

```bash
pytest tests/ -v --tb=short
```

**Expected output:**

```
tests/test_agents.py::TestAgentResponse::test_agent_response_creation PASSED
tests/test_agents.py::TestCoordinatorAgent::test_coordinator_execute_with_mock_agents PASSED
...
======================== 15 passed in 2.34s ========================
```

---

## ğŸ“ Course Alignment

This project implements key concepts from the **Kaggle Agents Intensive** course:

| Phase | Concept | Implementation |
|-------|---------|-----------------|
| **Day 1b** | Agent Architectures | Sequential Coordinator + Specialized Agents |
| **Day 2** | Tools & Integration | Tool Registry, MCP-compatible format |
| **Day 3a-3b** | Sessions & Memory | SessionManager with persistent storage |
| **Day 4a** | Observability & Logging | StructuredLogger + MetricsCollector |
| **Day 4b** | Agent Evaluation | Test cases, metrics, eval framework |

---

## ğŸ”„ Workflow (MVP)

1. **User Input** â†’ Business context + query
2. **Coordinator** â†’ Orchestrates analysis sequence
3. **Historical Analyzer** â†’ Detects past anomalies (mock)
4. **Live Monitor** â†’ Detects real-time issues (mock)
5. **Recommender** â†’ Generates recommendations (mock)
6. **Synthesis** â†’ Ranks issues by priority
7. **Session Save** â†’ Store findings for future reference
8. **Output** â†’ Structured report + next actions

---

## ğŸš§ MVP Limitations & Future Work

### Current (MVP Phase)
- âœ… Mock agents (placeholders, not using real Gemini API)
- âœ… CSV data only (no Sheets/API integration)
- âœ… Local session storage (no cloud persistence)
- âœ… Sequential execution only (no parallel agents)

### Next Phases (Day 2-7)
- ğŸ”² Real Gemini agent implementations
- ğŸ”² Tool integration (data ingestion, analysis, external APIs)
- ğŸ”² Real anomaly detection algorithms
- ğŸ”² Parallel agent execution
- ğŸ”² Cloud deployment (Google Cloud Run)
- ğŸ”² A2A communication between external agents
- ğŸ”² Advanced memory & feedback loops
- ğŸ”² Production-grade error handling

See `docs/DEPLOYMENT_ROADMAP.md` for detailed scaling strategy.

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/my-feature`)
3. Commit changes (`git commit -am 'Add feature'`)
4. Push to branch (`git push origin feature/my-feature`)
5. Create Pull Request

### Code Standards

- Follow PEP 8
- Add docstrings to all functions
- Include unit tests for new features
- Update README with new functionality
- Maintain >80% test coverage

---

## ğŸ“ License

This project is part of the **Kaggle Agents Intensive** capstone.
License: [Your License Here]

---

## ğŸ“ Support

- ğŸ“§ Email: [your-email@example.com]
- ğŸ› Issues: [GitHub Issues Link]
- ğŸ“š Documentation: See `docs/` directory

---

## ğŸ‰ Getting Started Checklist

- [ ] Clone repository
- [ ] Install dependencies (`pip install -r requirements.txt`)
- [ ] Review `config/agent_config.yaml`
- [ ] Run tests (`pytest tests/`)
- [ ] Try demo script or Notebook
- [ ] Explore `evaluation/` for metrics
- [ ] Read `ARCHITECTURE.md` for deep dive
- [ ] Plan Phase 2 improvements

---

**Last Updated:** 2024-11-21  
**Status:** âœ… MVP Complete - Ready for Phase 2 (Tool Integration)  
**Next Milestone:** Day 2 - Tool Integration & Real Agent Implementations
